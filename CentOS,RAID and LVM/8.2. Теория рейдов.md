
## **RAID-массивы**

**RAID** (англ. _Redundant Array of Independent Disks,_ избыточный массив независимых (самостоятельных) дисков) — технология виртуализации данных для объединения нескольких физических дисковых устройств в логический модуль для повышения отказоустойчивости и производительности.

Первая причина важности рейда — **сохранение информации, отказоустойчивость.**

Рейды бывают аппаратные и программные.

**Аппаратный рейд** — это рейд, организованный с помощью контроллера рейда, который может быть реализован на материнской плате или как отдельный модуль. Он создаётся вне операционной системы, и система в таком случае ставится уже на собранный рейд и работает на нём. Настраивается такой рейд так же, до загрузки системы. Контроллеры имеют текстовую или графическую оболочку, где рейд и настраивается. В ней диски делятся по группам, а в дальнейшем каждой группе назначается тип рейда.

**Программный рейд** создаётся уже внутри операционной системы, и строится на дисках, которые распознаёт сама операционная система. А так имеет такой же принцип построения рейдов, как и аппаратный. В данном юните мы будем более подробно рассматривать этот способ построения рейдов.

## **Уровни рейдов**

### **RAID 0**

Если некий большой файл разбит на блоки данных, и эти блоки данных разбиты по двум дискам, то они могут считываться параллельно, что позволяет ускорить работу в два раза. Но в этом рейде нет избыточности, а значит, он по сути не является рейдом. Если выйдет из строя хотя бы один диск, то информация на всех дисках будет утеряна.

**Итого:** от двух до _N_ дисков, нет избыточности, повышенная производительность.

![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/8ce0e7f4886e7a7b4ad317f302ca7b3a/asset-v1:SkillFactory+ADMIN+2020+type@asset+block/ADMIN_m8_u2_1.png)

### **RAID 1**

MIRROR (ЗЕРКАЛО)

Это «зеркальный» массив — один диск является точной копией другого. Дисков может быть от двух до _N_. Обеспечивает избыточность, а значит, повышает надёжность хранения данных.

![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/df96ef98d45bb4b76a5e2051ab32e602/asset-v1:SkillFactory+ADMIN+2020+type@asset+block/ADMIN_m8_u2_2.png)

### **RAID 2**

В случае данного рейда диски делятся на две группы — одна группа отвечает за хранение данных, вторая — за коррекцию ошибок. Т.е. если какая-то часть информации повреждается, то она может быть восстановлена с помощью группы, которая отвечает за коррекцию.

Дисков должно быть 2^n-1, где _n_ — это количество дисков, необходимое для группы, отвечающей за коррекцию данных. Группа хранения данных работает по такому же принципу, как и _RAID 0,_ то есть распределяет блоки данных фиксированной величины по дискам, что увеличивает скорость чтения и записи.

Недостаток этого рейда в том, что требуется **большое количество дисков,** обычно от 7. Это отказоустойчивый дисковый массив. В нём также используется так называемый [код Хэмминга](https://habr.com/ru/post/140611/) — избыточное кодирование. Код Хэмминга позволяет исправлять одиночные и обнаруживать двойные неисправности.

![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/0bf122251cf3974e517c2b23b792a26e/asset-v1:SkillFactory+ADMIN+2020+type@asset+block/ADMIN_m8_u2_3.png)

### **RAID 3**

Отказоустойчивый массив с параллельной передачей данных и четностью. В данном случае данные разбиваются на подблоки на уровне байт (в отличие от _RAID 2_), и после этого записываются одновременно на все диски, которые входят в массив, кроме последнего (n-1). Он используется для чётности.

Допустим, в запись поступает элемент данных _А_. Байт _А1_ будет записан на первый диск, байт _А2_ — на второй диск, байт _А3_ — на третий, а на четвёртый будет записан результат вычисления чётности записанных байт Ар. Процесс будет работать циклично, пока весь элемент данных _А_ не будет записан.

Но один из главных минусов этого рейда в том, что все диски массива должны работать синхронно при побайтовой записи данных, а значит, одновременно может обрабатываться только один запрос.

![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/1f72d9e02de4a47c15463a43006d93be/asset-v1:SkillFactory+ADMIN+2020+type@asset+block/ADMIN_m8_u2_4.png)

### **RAID 4**

Отказоустойчивый массив независимых дисков с разделяемым диском чётности. Главное отличие _RAID 4_ от _RAID 3_ заключается в том, что на четвёртом уровне расслоение данных выполняется на уровне секторов, а не на уровне байтов, что позволяет увеличить производительность, но снижает скорость при записи данных малого объема. Это связано с тем, что чётность генерируется при записи и записывается на единственный диск, а не на несколько.

![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/e3b55a4a89569ca3f28a65f9c675e684/asset-v1:SkillFactory+ADMIN+2020+type@asset+block/ADMIN_m8_u2_5.png)

### **RAID 5**

Отказоустойчивый массив независимых дисков с распределенной чётностью.

В отличие от предыдущего уровня, чётность хранится не на одном диске, а распределяется по всем дискам массива. Минимальное количество дисков — три. Может функционировать при выходе из строя одного диска.

Это один из самых популярных рейдов. Благодаря его производительности и надёжности он часто используется для хранения данных большого объёма, таких как видеонаблюдение. Из минусов можно добавить, что после того, как будет заменён неисправный диск, может долго идти пересборка.

![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/cbd1212b4e84f18b7c9a966d0bdcdb3d/asset-v1:SkillFactory+ADMIN+2020+type@asset+block/ADMIN_m8_u2_6.png)

### **RAID 6**

Массив, похожий на 5, но имеет повышенную надёжность. Применяет технологию чередования четности, но здесь вычисляются дважды и копируются на два разных блока. Это позволяет восстановить данные, даже если одновременно выходит из строя до трёх дисков.

![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/ed18d53d971c0cc0783b2846cbf3754b/asset-v1:SkillFactory+ADMIN+2020+type@asset+block/ADMIN_m8_u2_7.png)

## **Работа с программным рейдом**

Давайте подробнее обсудим работу с программным рейдом, а также попробуем его собрать, используя утилиту **mdadm** (сокращение от _multiple device administration_).

Утилита _mdadm_ обладает таким синтаксисом:

mdadm [mode] [array] [options]

Подробнее про эту команду можно почитать [здесь](https://linux.die.net/man/8/mdadm).

На виртуальном сервере, с которого я показываю примеры, были добавлены дополнительно 3 диска по 5 ГБ, каждый через _vmware_.

![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/7d65da5b22fe4b0a167df96257dad4d5/asset-v1:SkillFactory+ADMIN+2020+type@asset+block/ADMIN_m8_u2_8.png)

А после я перезагрузил операционную систему, чтобы диски инициировались и появились среди девайсов (можно делать это и без перезагрузки, и про это будет рассказано в следующих юнитах).

Итого у меня добавилось три диска: _sdc / sdd / sde._ На их основе и будет собираться рейд.

Создаём массив:

root@localhost:~#  mdadm --create --verbose /dev/md0 --level=5 --raid-devices=3 /dev/sdc /dev/sdd /dev/sde

Используем _create mode_ (режим) для создания массива. Флаг `--verbose` предписывает выводить больше информации (для понимания происходящего).

С помощью `--level` указывается уровень рейда. И в `--raid-devices` мы указываем количество устройств, а затем перечисляем их по абсолютному пути. В случае, если всё сделано было правильно, будут выведены следующие сообщения, где в конце будет написано `«mdadm: array /dev/md0 started»`, что означает, что началась сборка девайса. Размеры взяты по умолчанию.

mdadm: layout defaults to left-symmetric
mdadm: layout defaults to left-symmetric
mdadm: chunk size defaults to 512K
mdadm: size set to 5237760K
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.

Информацию о рейде можно посмотреть в `/proc/mdstat` с помощью `cat`.

root@localhost:~# cat /proc/mdstat
Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10]
md0 : active raid5 sde[3] sdd[1] sdc[0]
      10475520 blocks super 1.2 level 5, 512k chunk, algorithm 2 [3/3] [UUU]

unused devices: <none>

Здесь будет написано: `md0` активен, какой у него уровень, размер, какие диски задействованы и что они все в порядке (`[UUU]`).

mdadm --detail --scan
ARRAY /dev/md0 metadata=1.2 name=localhost:0 UUID=da50f588:2b7b24e0:d5add80d:5281df4f

Посмотреть, появился ли массив в системе, можно также и с помощью команды в `lsblk`.

sdc                         8:32   0    5G  0 disk
└─md0                       9:0    0   10G  0 raid5
sdd                         8:48   0    5G  0 disk
└─md0                       9:0    0   10G  0 raid5
sde                         8:64   0    5G  0 disk
└─md0                       9:0    0   10G  0 raid5

Я приложил сюда часть вывода. Видно, что поверх дисков записан _md0 RAID 5_ размером 10 ГБ из трёх дисков. Напомним, _RAID 5_ — это массив, у которого на диски пишутся как данные, так и контрольные суммы. Когда все диски симметричны, на контрольную сумму выделяется объём, равный одному диску. То есть тут три диска по 5 ГБ: под данные заложено 10 ГБ, под контрольные суммы — 5 ГБ.

И в дальнейшем с _md0_ требуется работать, как с обычным диском: создать на нём файловую систему, смонтировать её в нужное место.

### **Удаление диска**

Но в чём основная особенность рейда? Если я удалю из системы один диск, который входил в рейд (представим, что он вышел из строя), то будет показываться:

 mdadm --detail --scan
ARRAY /dev/md0 metadata=1.2 name=localhost:0 UUID=da50f588:2b7b24e0:d5add80d:5281df4f
root@localhost:/# cat /proc/mdstat
Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10]
md0 : active raid5 sde[3](F) sdd[1] sdc[0]
      10475520 blocks super 1.2 level 5, 512k chunk, algorithm 2 [3/2] [UU_]

unused devices: <none>

Рейд ещё активен, но в нём только два из трёх дисков, которые были (`[UU_]` — видно, что третьего диска нет). А также рядом с _sde_ появилась (_F_), означающая, что диск неисправный _(fail)._ Что в таком случае делать?

В таком случае необходимо вывести диск из массива (пометить как сбойный и удалить из массива), заменить его физически и ввести уже новый диск в массив:

root@localhost:~#  mdadm --manage /dev/md0 --fail /dev/sde
mdadm: set /dev/sde faulty in /dev/md0

Третий диск _(sde)_ в рейде помечается как сбойный через мод `manage` с указанием рейда и полного пути к диску.

root@localhost:~#  mdadm --manage /dev/md0 --remove /dev/sde
mdadm: hot removed /dev/sde from /dev/md0

Потом его удаляем через `--remove`.

И потом, вставив новый диск (он называется так же), добавляем его в список.

root@localhost:~#  mdadm --manage /dev/md0 --add /dev/sde
mdadm: added /dev/sde

Надо будет подождать, пока рейд пересобирается. После этого можно им будет пользоваться дальше без потери данных.

### **Добавление диска**

Увеличить массив можно тем же способом. Сначала в него добавляете новый диск:

root@localhost:~#  mdadm --manage /dev/md0 --add /dev/sdf
mdadm: added /dev/sdf

А потом указываете, что требуется расширение с помощью `--grow`.

root@localhost:~# mdadm --backup-file=/var/backup --grow /dev/md0 --raid-devices=4

Желательно указывать `--backup-file=/var/backup` на случай, если добавление и распределение данных неожиданно прервутся, чтобы реально было восстановить данные с помощью бэкапа.

А потом, как видите, мы указываем, что в рейде уже четыре устройства (`--raid-devices=4`).